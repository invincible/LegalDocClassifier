{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9deea82-ebd1-4b73-ba48-59bb8520849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fdac428-fc84-4558-ba40-1595389d402e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         class                                               text\n",
      "0  arrangement  СОГЛАШЕНИЕ N 8\\nо расторжении трудового догово...\n",
      "1  arrangement  Соглашение о предоставлении опциона на заключе...\n",
      "2  arrangement  Соглашение\\nо реструктуризации задолженности\\n...\n",
      "3  arrangement  Дополнительное соглашение\\r\\nк договору купли-...\n",
      "4  arrangement  Соглашение\\nо расторжении договора об оказании...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 501 entries, 0 to 500\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   class   501 non-null    object\n",
      " 1   text    501 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 8.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"dataset/sample.csv\")\n",
    "print(data.head())\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5604fc92-eff6-425b-8d66-41b0755a9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"text\"]\n",
    "y = data[\"class\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b311c603-31c7-41d4-ac99-0aef98758bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Сергей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Сергей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удаление пунктуации и спецсимволов\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Удаление числовых символов\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Токенизация\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Удаление стоп-слов и лемматизация\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    # Объединение токенов обратно в строку\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "# Применение предобработки к текстам\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9602080a-79ad-47b0-8e6d-726db7033855",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=10000, min_df=5, max_df=0.7, ngram_range=(1, 2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "048c7f8f-280b-47bd-bbc0-e5901dedd660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "20 fits failed out of a total of 40.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1461, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"D:\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 447, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "D:\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [   nan 0.6175    nan 0.9625    nan 0.98      nan 0.9825]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  {'C': 100, 'penalty': 'l2'}\n",
      "Accuracy: 0.9801980198019802\n",
      "Precision: 0.9738723872387239\n",
      "Recall: 0.9801980198019802\n",
      "F1-score: 0.9759547383309759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Определение сетки параметров\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "# Создание модели логистической регрессии\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Поиск по сетке параметров\n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5,  # 5-fold cross-validation\n",
    "                           scoring='accuracy',  # Оптимизируем по accuracy\n",
    "                           n_jobs=-1,  # Используем все доступные ядра процессора\n",
    "                           verbose=2) \n",
    "\n",
    "grid_search.fit(X_train_vec, y_train)\n",
    "\n",
    "# Лучшие найденные параметры\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "# Лучшая модель\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Оценка качества лучшей модели на тестовой выборке\n",
    "y_pred = best_model.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d182aec-26d2-4199-990a-1c7a22100b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 594 ms\n",
      "Wall time: 582 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=100, max_iter=1000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#modelLR = LogisticRegression(C=10, max_iter=1000)\n",
    "modelLR = LogisticRegression(C=100, penalty='l2', max_iter=1000)\n",
    "modelLR.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8fb6c7b2-b14f-4220-b1e1-c80f8e17ee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9801980198019802\n",
      "Precision: 0.9738723872387239\n",
      "Recall: 0.9801980198019802\n",
      "F1-score: 0.9759547383309759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = modelLR.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961746d-22b0-4324-bafd-d85fe188bd1b",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0243bd3-e03e-4ba9-8b22-34b8c0af7f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 2.2936170\ttotal: 335ms\tremaining: 1m 40s\n",
      "50:\tlearn: 0.6788766\ttotal: 16.6s\tremaining: 1m 21s\n",
      "100:\tlearn: 0.3516000\ttotal: 34.6s\tremaining: 1m 8s\n",
      "150:\tlearn: 0.2059833\ttotal: 54.2s\tremaining: 53.5s\n",
      "200:\tlearn: 0.1309667\ttotal: 1m 14s\tremaining: 36.6s\n",
      "250:\tlearn: 0.0960295\ttotal: 1m 34s\tremaining: 18.5s\n",
      "299:\tlearn: 0.0797905\ttotal: 1m 54s\tremaining: 0us\n",
      "CPU times: total: 12min 38s\n",
      "Wall time: 1min 55s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x298fc4566e0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Создание и обучение модели CatBoost\n",
    "modelCB = CatBoostClassifier(iterations=300, \n",
    "                           #eval_metric='Accuracy',\n",
    "                           learning_rate=0.05,\n",
    "                           random_seed=42,\n",
    "                           logging_level='Verbose',  # Устанавливаем уровень логирования\n",
    "                           metric_period=50,\n",
    "                           od_type='Iter',  # Тип детектора переобучения\n",
    "                           od_wait=100)  # Число итераций между проверками условия останова\n",
    "\n",
    "modelCB.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc92b782-6ed6-4db8-bfc2-923962b33098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9405940594059405\n",
      "Precision: 0.9372372763592149\n",
      "Recall: 0.9405940594059405\n",
      "F1-score: 0.9345106875379596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Предсказания на тестовой выборке\n",
    "y_pred = modelCB.predict(X_test_vec)\n",
    "\n",
    "# Расчет метрик\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e53253-4a5a-4f91-9fcb-9d8a329d5aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Определение сетки параметров\n",
    "param_grid = {\n",
    "    'depth': [4, 6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'iterations': [100, 300, 500],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7, 9]\n",
    "}\n",
    "\n",
    "# Создание модели CatBoost\n",
    "model = CatBoostClassifier(eval_metric='Accuracy',  # Изменяем метрику на Accuracy\n",
    "                           logging_level='Verbose',  # Устанавливаем уровень логирования\n",
    "                           metric_period=50,\n",
    "                           random_seed=42)\n",
    "\n",
    "# Поиск по сетке параметров\n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5,  # 5-fold cross-validation\n",
    "                           scoring='accuracy',  # Оптимизируем по Accuracy\n",
    "                           n_jobs=-1,  # Используем все доступные ядра процессора\n",
    "                           verbose=2) \n",
    "\n",
    "grid_search.fit(X_train_vec, y_train)\n",
    "\n",
    "# Лучшие найденные параметры\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "\n",
    "# Лучшая модель\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Оценка качества лучшей модели на тестовой выборке\n",
    "y_pred = best_model.predict(X_test_vec)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred, average=\"weighted\"))\n",
    "print(\"F1-score:\", f1_score(y_test, y_pred, average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618e582b-9b7d-4710-b51c-68bf454021b5",
   "metadata": {},
   "source": [
    "Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3c2b22d-4e4f-43f0-b34e-7da7746a5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7081c7a-fc67-49f1-9ce5-566f2f4aac85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06965e3e-77e7-42c9-ae3b-4580f46580a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train.tolist(), truncation=True, padding='max_length', max_length=512)\n",
    "test_encodings = tokenizer(X_test.tolist(), truncation=True, padding='max_length', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4f7ad63-4aa7-4f61-acf8-d502e6d285b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased-sentence and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████████████████████| 50/50 [1:01:25<00:00, 73.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 average training loss: 1.92\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'flat_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 81\u001b[0m\n\u001b[0;32m     79\u001b[0m         logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     80\u001b[0m         label_ids \u001b[38;5;241m=\u001b[39m b_labels\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 81\u001b[0m         accuracies\u001b[38;5;241m.\u001b[39mappend(\u001b[43mflat_accuracy\u001b[49m(logits, label_ids))\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(accuracies)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Оценка на тестовой выборке\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'flat_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Загрузка предобученной модели и токенизатора\n",
    "model_name = 'DeepPavlov/rubert-base-cased-sentence'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=11)  # 11 классов\n",
    "\n",
    "# Перевод текстов в последовательности токенов\n",
    "def tokenize(texts, max_length=512):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            text,\n",
    "                            add_special_tokens=True,\n",
    "                            max_length=max_length,\n",
    "                            pad_to_max_length=True,\n",
    "                            return_attention_mask=True,\n",
    "                            return_tensors='pt')\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
    "\n",
    "# Токенизация текстов\n",
    "train_input_ids, train_attention_masks = tokenize(X_train.tolist())\n",
    "val_input_ids, val_attention_masks = tokenize(X_test.tolist())\n",
    "\n",
    "# Создание датасетов и даталоадеров\n",
    "#train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, torch.tensor(y_train.values))\n",
    "train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, torch.tensor(y_train, dtype=torch.long))\n",
    "train_dataloader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=8)\n",
    "\n",
    "#val_dataset = torch.utils.data.TensorDataset(val_input_ids, val_attention_masks, torch.tensor(y_test.values))\n",
    "val_dataset = torch.utils.data.TensorDataset(val_input_ids, val_attention_masks, torch.tensor(y_test, dtype=torch.long))\n",
    "val_dataloader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=8)\n",
    "\n",
    "# Перенос модели на GPU, если доступно\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Оптимизатор и планировщик скорости обучения\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\n",
    "epochs = 5\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "# Цикл обучения\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader, desc=f'Epoch {epoch+1}'):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f'Epoch {epoch+1} average training loss: {avg_train_loss:.2f}')\n",
    "\n",
    "    model.eval()\n",
    "    accuracies = []\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "        logits = outputs.logits\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        accuracies.append(flat_accuracy(logits, label_ids))\n",
    "    print(f'Validation accuracy: {np.mean(accuracies):.2f}')\n",
    "\n",
    "# Оценка на тестовой выборке\n",
    "model.eval()\n",
    "predictions , true_labels = [], []\n",
    "for batch in val_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)\n",
    "\n",
    "print(f'Accuracy: {flat_accuracy(np.concatenate(predictions, axis=0), np.concatenate(true_labels, axis=0)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea5d87e-9c37-4728-8b4e-41dcd66bfcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install --upgrade urllib3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f33d073b-50cf-4033-9253-07580b99f5e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     labels_flat \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(pred_flat \u001b[38;5;241m==\u001b[39m labels_flat) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels_flat)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflat_accuracy(np\u001b[38;5;241m.\u001b[39mconcatenate(\u001b[43mpredictions\u001b[49m,\u001b[38;5;250m \u001b[39maxis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\u001b[38;5;250m \u001b[39mnp\u001b[38;5;241m.\u001b[39mconcatenate(true_labels,\u001b[38;5;250m \u001b[39maxis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "print(f'Accuracy: {flat_accuracy(np.concatenate(predictions, axis=0), np.concatenate(true_labels, axis=0)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0bf2fff-24da-45c1-90d9-00bc40ff599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions , true_labels = [], []\n",
    "for batch in val_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "    logits = outputs.logits\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    predictions.append(logits)\n",
    "    true_labels.append(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cd851ad-2ba6-4128-80d1-f0599e2c57ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.88\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {flat_accuracy(np.concatenate(predictions, axis=0), np.concatenate(true_labels, axis=0)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524a385-601f-4f0d-a389-bf84b8c90c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
